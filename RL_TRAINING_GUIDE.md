# Reinforcement Learning Guide for CoreWars8086 Counter-Strategy

## Overview

Yes, it's absolutely possible to use reinforcement learning (RL) to train an AI to build counter-strategies against 7 known static opponents in CoreWars8086! This guide outlines multiple approaches and implementation strategies.

## Why RL is Suitable

1. **Static Opponents**: Since opponents don't change, the RL agent can learn deterministic counter-strategies
2. **Clear Win Condition**: Last warrior standing provides clear reward signals
3. **Reproducible Environment**: The CoreWars engine provides a deterministic simulation environment
4. **Fast Iteration**: Battles can run quickly, enabling many training episodes

## Approach 1: Neural Program Synthesis (Recommended)

### Concept
Train a neural network to generate warrior bytecode directly. The network learns to output a sequence of bytes (up to 512 bytes) that form a winning warrior.

### Architecture

```
Input (State) → Encoder → Decoder → Bytecode Output (Action)
```

**State Representation:**
- Opponent bytecode embeddings (7 opponents × code features)
- Current round number (normalized)
- Memory statistics (if observable)
- Historical win rates against each opponent

**Action Space:**
- Sequence of bytes (0-255) up to 512 bytes
- Or: Sequence of 8086 instruction templates with parameters

**Reward Structure:**
- **Win**: +1000 points
- **Survival per round**: +1 point (encourages longevity)
- **Death**: -100 points
- **Opponent elimination**: +50 points per opponent killed
- **Final score**: +500 if last warrior standing

### Implementation Steps

1. **Create RL Environment Wrapper**
   - Wrap the CoreWars engine as a Gym-style environment
   - Implement `reset()`, `step(action)`, `render()` methods
   - Action = generated bytecode array

2. **Neural Network Architecture**
   - Use Transformer or LSTM for sequence generation
   - Output layer: 256-way softmax (one per byte value)
   - Or: Use VAE (Variational Autoencoder) for structured code generation

3. **Training Algorithm**
   - **PPO (Proximal Policy Optimization)**: Good balance of stability and sample efficiency
   - **A3C/A2C**: Can leverage parallel battles
   - **Evolution Strategies**: Alternative if gradient-based methods struggle

4. **Training Loop**
   ```
   For each episode:
     1. Generate warrior bytecode from current policy
     2. Load 7 static opponents + generated warrior
     3. Run battle to completion
     4. Calculate reward based on outcome
     5. Update policy network
   ```

## Approach 2: Template-Based Generation

### Concept
Instead of raw bytecode, generate warriors from a set of instruction templates. This reduces the action space and makes learning more tractable.

### Instruction Templates
- MOV, ADD, SUB, JMP, CALL, RET
- Conditional jumps (JZ, JNZ, JC, etc.)
- Memory operations
- Loop structures

### Action Space
- Sequence of instruction choices with operands
- Much smaller than raw bytecode (e.g., ~1000 possible actions vs 256^512)

### Advantages
- Faster convergence
- More interpretable strategies
- Easier to add domain knowledge

## Approach 3: Hierarchical RL

### Concept
Two-level hierarchy:
1. **High-level**: Strategic decisions (aggressive, defensive, stealth, etc.)
2. **Low-level**: Instruction generation based on strategy

### Benefits
- Separates strategy from implementation
- Can pre-train low-level on instruction sequences
- High-level learns when to apply which strategy

## Implementation Architecture

### 1. Environment Wrapper (Python)

```python
import jpype
from gym import Env
import numpy as np

class CoreWarsEnv(Env):
    def __init__(self, opponent_paths):
        # Initialize JVM and load CoreWars engine
        self.opponents = opponent_paths  # 7 static opponents
        self.max_code_size = 512
        
    def reset(self):
        # Generate initial random warrior or use current policy
        return self._get_state()
    
    def step(self, action):
        # action: bytecode array (up to 512 bytes)
        # Run battle with 7 opponents + generated warrior
        result = self._run_battle(action)
        reward = self._calculate_reward(result)
        done = result['finished']
        return self._get_state(), reward, done, result
    
    def _run_battle(self, warrior_code):
        # Use JPype to call Java CoreWars engine
        # Return battle statistics
        pass
```

### 2. Policy Network (PyTorch/TensorFlow)

```python
import torch
import torch.nn as nn

class WarriorGenerator(nn.Module):
    def __init__(self, state_dim=128, hidden_dim=256):
        super().__init__()
        self.encoder = nn.LSTM(state_dim, hidden_dim, num_layers=2)
        self.decoder = nn.Linear(hidden_dim, 256)  # 256 possible byte values
        
    def forward(self, state, max_length=512):
        # Generate bytecode sequence
        encoded = self.encoder(state)
        bytecode = []
        for _ in range(max_length):
            byte_probs = torch.softmax(self.decoder(encoded), dim=-1)
            byte = torch.multinomial(byte_probs, 1)
            bytecode.append(byte)
        return torch.stack(bytecode)
```

### 3. Training Script

```python
from stable_baselines3 import PPO
from corewars_env import CoreWarsEnv

# Initialize environment
env = CoreWarsEnv(opponent_paths=['opp1', 'opp2', ..., 'opp7'])

# Train with PPO
model = PPO('MlpPolicy', env, verbose=1, 
            learning_rate=3e-4, n_steps=2048)
model.learn(total_timesteps=1_000_000)

# Save trained model
model.save('corewars_counter_strategy')
```

## State Representation Design

### Option A: Opponent Code Embeddings
- Use autoencoder to compress each opponent's bytecode
- Concatenate 7 opponent embeddings
- Add battle statistics (round number, alive count, etc.)

### Option B: Memory Snapshot
- Sample key memory regions
- CPU state (registers, flags)
- Opponent positions (if detectable)

### Option C: Historical Features
- Win rate against each opponent
- Average survival time
- Common failure modes

## Reward Shaping

### Basic Rewards
```python
def calculate_reward(result):
    if result['won']:
        return 1000.0
    elif result['died']:
        return -100.0
    else:
        # Still alive
        return result['rounds_survived'] * 0.1
```

### Advanced Rewards (Better Learning Signal)
```python
def calculate_reward(result):
    base_reward = 0
    
    # Win bonus
    if result['won']:
        base_reward += 1000
    
    # Survival bonus (diminishing returns)
    base_reward += np.log(result['rounds_survived'] + 1) * 10
    
    # Opponent elimination bonus
    base_reward += result['opponents_killed'] * 50
    
    # Death penalty
    if result['died']:
        base_reward -= 100
    
    # Efficiency bonus (win quickly)
    if result['won']:
        base_reward += max(0, 200 - result['rounds_survived']) * 0.5
    
    return base_reward
```

## Training Strategy

### Phase 1: Exploration (Early Training)
- High exploration rate (ε = 0.9)
- Random initialization
- Focus on discovering viable code patterns

### Phase 2: Exploitation (Mid Training)
- Reduce exploration (ε = 0.3)
- Fine-tune successful strategies
- Specialize against specific opponents

### Phase 3: Optimization (Late Training)
- Low exploration (ε = 0.1)
- Optimize code size and efficiency
- Generalize across all 7 opponents

## Practical Implementation Steps

### Step 1: Set Up Python-Java Bridge
```bash
pip install jpype1
```

### Step 2: Create Environment Wrapper
- Wrap Competition.java and War.java
- Expose battle execution as Python functions
- Handle state extraction and reward calculation

### Step 3: Implement Policy Network
- Start with simple LSTM/Transformer
- Output bytecode probabilities
- Use teacher forcing during initial training

### Step 4: Train with RL Algorithm
- Use Stable-Baselines3 or Ray RLlib
- Start with PPO (most stable)
- Experiment with A3C for parallelization

### Step 5: Evaluation and Iteration
- Test against 7 opponents
- Analyze generated warriors (disassemble)
- Refine reward function based on behavior

## Tools and Libraries

### RL Frameworks
- **Stable-Baselines3**: Easy to use, good documentation
- **Ray RLlib**: Scalable, supports distributed training
- **TensorFlow Agents**: Flexible, research-oriented

### Neural Network Libraries
- **PyTorch**: Recommended for research
- **TensorFlow**: Good production support

### Java-Python Integration
- **JPype**: Call Java from Python
- **Py4J**: Alternative JVM bridge

## Challenges and Solutions

### Challenge 1: Sparse Rewards
**Problem**: Most generated warriors die immediately, providing little learning signal.

**Solutions**:
- Reward shaping (survival bonuses)
- Curriculum learning (start with easier opponents)
- Intrinsic motivation (curiosity-driven exploration)

### Challenge 2: Large Action Space
**Problem**: 256^512 possible warriors is intractable.

**Solutions**:
- Template-based generation
- Hierarchical actions
- VAE for structured generation

### Challenge 3: Non-Differentiable Environment
**Problem**: Can't backpropagate through battle execution.

**Solutions**:
- Policy gradient methods (PPO, A3C)
- Evolution strategies
- REINFORCE with variance reduction

### Challenge 4: Evaluation Time
**Problem**: Running full battles is slow.

**Solutions**:
- Parallel battle execution
- Early termination for obviously losing warriors
- Battle simulation caching

## Expected Timeline

- **Week 1**: Environment wrapper, basic policy network
- **Week 2**: Initial training runs, reward tuning
- **Week 3**: Iteration and optimization
- **Week 4**: Evaluation and refinement

## Success Metrics

- **Win Rate**: >50% against all 7 opponents combined
- **Survival Rate**: >80% survive past round 1000
- **Code Efficiency**: <256 bytes average warrior size
- **Generalization**: Works across different random seeds

## Next Steps

1. **Start Simple**: Begin with template-based generation
2. **Validate Environment**: Ensure CoreWars wrapper works correctly
3. **Baseline**: Train against 1 opponent first, then scale up
4. **Iterate**: Refine based on generated warrior analysis

## Example: Minimal Working Prototype

See `rl_training/` directory for a complete implementation skeleton.

## Quick Start

```bash
# 1. Install dependencies
cd rl_training
pip install -r requirements.txt

# 2. Compile Java engine
cd ..
mvn compile

# 3. Prepare 7 opponent warriors
# Place them in a directory (e.g., opponents/)

# 4. Train (mock mode for testing)
cd rl_training
python train.py --opponents ../opponents/opp1 ... ../opponents/opp7 --mock

# 5. Train (full mode)
python train.py --opponents ../opponents/opp1 ... ../opponents/opp7 \
    --timesteps 1000000
```

## Additional Resources

- **CoreWars8086 Documentation**: See README.md
- **Stable-Baselines3 Docs**: https://stable-baselines3.readthedocs.io/
- **JPype Documentation**: https://jpype.readthedocs.io/
